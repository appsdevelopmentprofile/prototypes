# -*- coding: utf-8 -*-
"""Frontend_module1_DocIntelligence.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e9Vxr3k3t3yBANt3U1d09ZCNUOgKvEyK
"""

pip install nbmanips

!nb select has_html_tag h1 | nb split -s nb.ipynb

"""Libraries



"""

pip install PyMuPDF

pip install kaggle pymupdf tensorflow networkx matplotlib numpy

"""# Step 0: Install Kaggle and Download Dataset"""

from google.colab import drive
drive.mount('/content/drive')  # Mount to the standard Google Drive mount point in Colab
file_path = '/content/drive/MyDrive/Customs-Clearance-Form.pdf'  # Replace 'your_file_name.pdf' with the actual path to your file inside your Google Drive

# Step 0: Install Kaggle and Download Dataset
!pip install kaggle
!kaggle datasets download pranay27sy/maritime-documents-tags-classification

# Step 1: Extract Dataset
import zipfile
import os

with zipfile.ZipFile('maritime-documents-tags-classification.zip', 'r') as zip_ref:
    zip_ref.extractall('maritime_docs')

# Step 2: Load and Process PDFs using OCR
import fitz  # PyMuPDF
import os

# Function to extract text from a PDF file
def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    text = ''
    for page in doc:
        text += page.get_text()
    return text

# Example usage: Extract text from a sample PDF
sample_pdf_path = '/content/drive/MyDrive/Customs-Clearance-Form.pdf'   # Replace with actual path
extracted_text = extract_text_from_pdf(sample_pdf_path)
print('Extracted Text:', extracted_text)

"""STEP 3"""

# Step 3: Preprocess Text Data and Construct Graph
import networkx as nx
import matplotlib.pyplot as plt

# Sample preprocessing function to extract entities/tags from text
def preprocess_text(text):
    # Assuming simple splitting into sections or extracting key terms
    # In real use, you would extract meaningful document sections or tags
    return ['Title', 'Owner', 'Date', 'Total']  # Replace with actual preprocessing logic

# Function to build graph from tags
def build_graph_from_tags(tags):
    G = nx.Graph()
    for tag in tags:
        G.add_node(tag)
    # Example: Adding edges (this logic should depend on document structure)
    G.add_edges_from([('Title', 'Owner'), ('Owner', 'Date'), ('Date', 'Total')])
    return G

# Example usage: Build graph from extracted tags
extracted_tags = preprocess_text(extracted_text)
G = build_graph_from_tags(extracted_tags)

# Visualize the graph
nx.draw(G, with_labels=True, node_color='lightblue', node_size=2000)
plt.show()

# Step 4: Define GCN Model
import tensorflow as tf
import numpy as np

class GCN(tf.keras.Model):
    def __init__(self, num_nodes, num_features, output_dim):
        super(GCN, self).__init__()
        self.dense1 = tf.keras.layers.Dense(16, activation='relu')
        self.dense2 = tf.keras.layers.Dense(output_dim)

    def call(self, inputs):
        x, adjacency_matrix = inputs
        x = tf.matmul(adjacency_matrix, x)  # Propagate through adjacency matrix
        x = self.dense1(x)
        return self.dense2(x)

# Example node features and adjacency matrix (random data)
node_features = np.array([
    [1, 0],  # Title
    [0, 1],  # Owner
    [1, 1],  # Date
    [0, 0],  # Total
])

adj_matrix = np.array([
    [1, 1, 0, 0],
    [1, 1, 1, 0],
    [0, 1, 1, 1],
    [0, 0, 1, 1]
])

# Labels (e.g., 0 for 'Other', 1 for 'Key Entity')
labels = np.array([1, 0, 1, 0])

# Initialize and compile the GCN model
gcn_model = GCN(num_nodes=4, num_features=2, output_dim=2)
gcn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the GCN model
gcn_model.fit([node_features, adj_matrix], labels, epochs=10)

# Step 5: Save Pre-trained Model
gcn_model.save('gcn_maritime_model.keras')
print('Model saved as gcn_maritime_model.keras')

"""# Step 6: Load and Use Pre-trained Model for Inference"""

pip install tensorflow numpy matplotlib networkx

import tensorflow as tf
import numpy as np
from tensorflow.keras.utils import register_keras_serializable

@register_keras_serializable()
class GCN(tf.keras.Model):
    def __init__(self, num_nodes, num_features, output_dim, **kwargs):
        super(GCN, self).__init__(**kwargs)
        self.num_nodes = num_nodes
        self.num_features = num_features
        self.output_dim = output_dim
        self.dense1 = tf.keras.layers.Dense(16, activation='relu')
        self.dense2 = tf.keras.layers.Dense(output_dim)

    def call(self, inputs):
        x, adjacency_matrix = inputs
        x = tf.matmul(adjacency_matrix, x)  # Propagate through adjacency matrix
        x = self.dense1(x)
        return self.dense2(x)

    def get_config(self):
        config = super(GCN, self).get_config()
        config.update({
            'num_nodes': self.num_nodes,
            'num_features': self.num_features,
            'output_dim': self.output_dim,
        })
        return config

    @classmethod
    def from_config(cls, config):
        return cls(**config)

"""# STREAMLIT"""

pip install streamlit tensorflow numpy matplotlib networkx pillow pytesseract

"""Using Streamlit and NMP to deploy"""

!pip install -q streamlit

!npm install localtunnel

import streamlit as st
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import networkx as nx
from PIL import Image
import pytesseract

# Define GCN class as in the backend code
class GCN(tf.keras.Model):
    def __init__(self, num_nodes, num_features, output_dim):
        super(GCN, self).__init__()
        self.dense1 = tf.keras.layers.Dense(16, activation='relu')
        self.dense2 = tf.keras.layers.Dense(output_dim)

    def call(self, inputs):
        x, adjacency_matrix = inputs
        x = tf.matmul(adjacency_matrix, x)  # Propagate through adjacency matrix
        x = self.dense1(x)
        return self.dense2(x)

# Streamlit UI
st.title("Intelligent Document Reader")

# Upload image
uploaded_file = st.file_uploader("Choose an image...", type=["png", "jpg", "jpeg"])

if uploaded_file is not None:
    # Load and display image
    image = Image.open(uploaded_file)
    st.image(image, caption='Uploaded Image', use_column_width=True)

    # Perform OCR
    st.write("Extracting text...")
    extracted_text = pytesseract.image_to_string(uploaded_file)
    st.write("Extracted Text:")
    st.text(extracted_text)

    # Build graph for GCN
    text_data = {'Seller': 'StoreX', 'Address': '123 Main St', 'Date': '2024-10-20', 'Total': '$100'}
    G = nx.Graph()

    # Simulated text positions for the graph construction
    text_positions = {
        'Seller': (1, 1), 'Address': (2, 1), 'Date': (3, 1), 'Total': (4, 1)
    }

    # Adding nodes and edges to graph
    for entity, position in text_positions.items():
        G.add_node(entity, pos=position)
    G.add_edges_from([('Seller', 'Address'), ('Address', 'Date'), ('Date', 'Total')])

    # Visualize the graph
    pos = nx.get_node_attributes(G, 'pos')
    plt.figure()
    nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=2000)
    plt.title("Graph Representation")
    st.pyplot(plt)

    # Prepare node features and adjacency matrix for GCN
    node_features = np.array([
        [1, 0],  # Seller
        [0, 1],  # Address
        [1, 1],  # Date
        [0, 0],  # Total
    ])

    adj_matrix = np.array([
        [1, 1, 0, 0],
        [1, 1, 1, 0],
        [0, 1, 1, 1],
        [0, 0, 1, 1]
    ])

    # Initialize and run GCN model
    gcn_model = GCN(num_nodes=4, num_features=2, output_dim=2)
    node_outputs = gcn_model([node_features, adj_matrix])

    # Display predicted classes
    class_names = ['Other', 'Key Entity']
    predicted_classes = tf.argmax(node_outputs, axis=1)

    st.write("Predicted Classes:")
    for i, node in enumerate(G.nodes()):
        st.write(f"Node: {node}, Predicted Class: {class_names[predicted_classes[i].numpy()]}")

!wget -qO- http://ipinfo.io/ip

# Step 1: Install the necessary packages
!pip install streamlit tensorflow numpy matplotlib networkx pillow pytesseract pyngrok

# Step 2: Write your Streamlit app to a file
with open("app.py", "w") as f:
    f.write("""
import streamlit as st
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import networkx as nx
from PIL import Image
import pytesseract

# Define GCN class
class GCN(tf.keras.Model):
    def __init__(self, num_nodes, num_features, output_dim):
        super(GCN, self).__init__()
        self.dense1 = tf.keras.layers.Dense(16, activation='relu')
        self.dense2 = tf.keras.layers.Dense(output_dim)

    def call(self, inputs):
        x, adjacency_matrix = inputs
        x = tf.matmul(adjacency_matrix, x)
        x = self.dense1(x)
        return self.dense2(x)

# Streamlit UI
st.title("Intelligent Document Reader")

# Upload image
uploaded_file = st.file_uploader("Choose an image...", type=["png", "jpg", "jpeg"])

if uploaded_file is not None:
    # Load and display image
    image = Image.open(uploaded_file)
    st.image(image, caption='Uploaded Image', use_column_width=True)

    # Perform OCR
    st.write("Extracting text...")
    extracted_text = pytesseract.image_to_string(image)
    st.write("Extracted Text:")
    st.text(extracted_text)

    # Build graph for GCN
    text_data = {'Seller': 'StoreX', 'Address': '123 Main St', 'Date': '2024-10-20', 'Total': '$100'}
    G = nx.Graph()

    # Simulated text positions for the graph construction
    text_positions = {
        'Seller': (1, 1), 'Address': (2, 1), 'Date': (3, 1), 'Total': (4, 1)
    }

    # Adding nodes and edges to graph
    for entity, position in text_positions.items():
        G.add_node(entity, pos=position)
    G.add_edges_from([('Seller', 'Address'), ('Address', 'Date'), ('Date', 'Total')])

    # Visualize the graph
    pos = nx.get_node_attributes(G, 'pos')
    plt.figure()
    nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=2000)
    plt.title("Graph Representation")
    st.pyplot(plt)

    # Prepare node features and adjacency matrix for GCN
    node_features = np.array([
        [1, 0],  # Seller
        [0, 1],  # Address
        [1, 1],  # Date
        [0, 0],  # Total
    ])

    adj_matrix = np.array([
        [1, 1, 0, 0],
        [1, 1, 1, 0],
        [0, 1, 1, 1],
        [0, 0, 1, 1]
    ])

    # Initialize and run GCN model
    gcn_model = GCN(num_nodes=4, num_features=2, output_dim=2)
    node_outputs = gcn_model([node_features, adj_matrix])

    # Display predicted classes
    class_names = ['Other', 'Key Entity']
    predicted_classes = tf.argmax(node_outputs, axis=1)

    st.write("Predicted Classes:")
    for i, node in enumerate(G.nodes()):
        st.write(f"Node: {node}, Predicted Class: {class_names[predicted_classes[i].numpy()]}")
""")

# Step 3: Run the app using pyngrok
from pyngrok import ngrok

# Open a tunnel to the streamlit port 8501
public_url = ngrok.connect(8501)
print('Your Streamlit app is live at:', public_url)

# Run the Streamlit app
!streamlit run app.py &

# Step 1: Install the necessary packages
!pip install pyngrok


import os
from threading import Thread
from pyngrok import ngrok

# Add your ngrok token here (if you have one)
ngrok.set_auth_token('2nzXiUhx92WYfJOij8rYnktu9S7_7ctF2X1ZJPtDnNX61iVKy')

def run_streamlit():
    # Change the port if 8501 is already in use or if you prefer another port
    os.system('streamlit run /content/frontend_module1_docintelligence.py --server.port 8501')

# Start a thread to run the Streamlit app
thread = Thread(target=run_streamlit)
thread.start()

# Open a tunnel to the streamlit port 8501
public_url = ngrok.connect(addr='8501', proto='http', bind_tls=True)
print('Your Streamlit app is live at:', public_url)